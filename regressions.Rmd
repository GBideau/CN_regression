---
title: "#CN_regressions\nLes communes nouvelles françaises (2012-2024) : proposition d'un modèle explicatif à l'aide de régressions logistiques"
author: "Gabriel Bideau"
date: '`r format(Sys.time(), "%d %B %Y %X")`' # %X pour rajouter l'heure
# bibliography: biblio/biblio.bib
link_citations: true
output:
     html_document:
       toc: true
       theme: united
       css : css/styles.css
       number_sections: true
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, # Afficher ou non le code R dans le document
                      eval=TRUE, #	Exécuter ou non le code R à la compilation
                      include=TRUE, #	Inclure ou non le code R et ses résultats dans le document
                      # results	“hide”/“asis”/“markup”/“hold”	Type de résultats renvoyés par le bloc de code
                      warning=TRUE, # Afficher ou non les avertissements générés par le bloc
                      message=TRUE, # Afficher ou non les messages générés par le bloc
                      cache=TRUE) # Utiliser le cache pour accélerer les knits.

# Librairies utiles
library(sf)
library(cartography)
library(mapsf)
library(readxl) # Pour import .xls
library(foreign) # Pour import .dbf
# library(dplyr) # Pour l'agrégation des données
library(flextable) # Pour la création de tableaux
library(knitr)
# remotes::install_github("antuki/COGugaison")
library(COGugaison) # Pour observer les millésimes des données
library(stringr) # Pour manipuler les chaînes de caractères
library(units) # Pour modifier les unités
library(condformat) # Pour formater les sorties des tableaux
library(forcats)
library(ggplot2) # Pour les graphiques
library(rstatix)
library(questionr)
library(corrplot) # Pour visualisation des corrélations
library(MTA) # Pour approche multiscalaire des inégalités ( https://cran.r-project.org/web/packages/MTA/vignettes/MTA_Scenario.html)

# Pour analyse factorielle et CAH
library(FactoMineR) 
library(factoextra)
library(cluster)
library(reshape)
library(reshape2)

# Pour visualisation des résultats de la régression logistique
library(gtsummary) 
theme_gtsummary_language("fr", decimal.mark = ",", big.mark = " ") # Pour afficher en français, avec les bonnes décimales etc
library(broom)
library(GGally)
library(effects)
library(forestmodel)
library(ggeffects)
library(texreg) # Pour rendus de tableaux de logit
library(labelled) # Pour labelliser les données
library(cowplot) # Pour visualisation données également de la CAH

library(spdep) # Pour les matrices de contiguïté
library(rgeoda) # Pour les matrices de contiguïté

library(leaps) # Pour régressions pas à pas

library(car) # Pour étude résidus régression linéaire
library(lmtest) # Pour étude résidus régression linéaire
library(arm) # Pour étude résidus logit


# library(rJava) # L'installation du package, nécessaire pour glmulti, ne fonctionne pas
# install.packages('glmulti', repos='http://cran.us.r-project.org', dependencies=TRUE) # Ne fonctionne pas
# download.file("https://cran.r-project.org/src/contrib/glmulti_1.0.8.tar.gz", "packages/glmulti_1.0.8.tar.gz")
# install.packages('glmulti', repos="packages/glmulti_1.0.8.tar.gz", dependencies=TRUE)
# library(glmulti) # Pour algorithme génétique de régression pas à pas


# Liste pour installer les packages si besoin :
# sf cartography mapsf readxl foreign dplyr flextable knitr stringr units condformat forcats ggplot2 rstatix questionr corrplot gtsummary broom GGally effects forestmodel ggeffects labelled cowplot spdep rgeoda

# Modification du séparateur de décimales
# options("digits"=7, "scipen"=0, OutDec = ",") 
options(max.print=100)
```

# Présentation

L'objectif de ce script est de tester différents modèles de régressions logistiques permettant de modéliser le phénomène des communes nouvelles.

Ce travail s'inscrit dans le cadre de la thèse de G. Bideau sur les communes nouvelles, sous la direction de R. Le Goix.

Plan envisagé au départ :

- Import de différentes données (par blocs complets)

- Prévoir des tests sur des sous-ensembles (Maine-et-Loire, Savoies, Ouest...)Il est important voire intéressant de cartographier ou de représenter graphiquement les résidus. Cela permet potentiellement de valider ou invalider le modèle mais également de réfléchir aux espaces où ce modèle fonctionne le mieux ou le moins bien.

- Calculs colinéarité, corrélations

- Méthode de régression classique (copier-coller analyse budgets)

- Forward stepwise ou backward stepwise

- Cartographie des résidus (y compris avec indice de Moran pour une auto-corrélation spatiale des résidus ?)


# Import des données

## Géométries

On n'importe au départ que les données à la géométrie 2011, mentionnant les futures communes nouvelles de rattachement.

```{r import_data_geom_2011}
# Import des géométries 2011
geom2011 <- st_read("Archives/data_prep 2011-2020(01)/data/geom.gpkg", layer = "geom2011", quiet = TRUE) 
geomfus2011 <- st_read("Archives/data_prep 2011-2020(01)/data/geom.gpkg", layer = "geomfus2011", quiet = TRUE) 
dep <- st_read("Archives/data_prep 2011-2020(01)/data/geom.gpkg", layer = "dep", quiet = TRUE)
geom_new <- st_read("Archives/data_prep 2011-2020(01)/data/geom.gpkg", layer = "geom_new", quiet = TRUE) 
geomCN_new <- st_read("Archives/data_prep 2011-2020(01)/data/geom.gpkg", layer = "geomCN_new", quiet = TRUE)  
```

## Base DAC

Base DAC [bideau2022b]

```{r import_data_DAC}
# Import des données de la base DAC
load("data/refdata.Rdata")

variables_RT <- colnames(df2011[which(str_detect(colnames(df2011), "_RT") == TRUE)])

```

## Budgets

```{r import_data_budgets_2011}
# Import des données budgétaires pour l'année 2011
load("data/refdata_budgets_bloccommunal_tot.Rdata")
rm(df_budgets_new, budgets_com_2011, budgets_com_2020)

# Fusion des données budgétaires avec les autres données
# On liste les variables renseignées dans les différents df
colbudgets2011 <- colnames(df_budgets_2011)

coldf2011 <- colnames(df2011)

# On liste les variables présentes seulement dans budgets (celles qu'il va falloir y prélever)
colgarder2011 <- setdiff(colbudgets2011, coldf2011)

# On fusionne les données de data_prep et des budgets, en ne gardant pour ces dernières que les données précédemment identifiées
df2011 <- merge(df2011,
                     df_budgets_2011[, c("CODGEO", colgarder2011)],
                     by = "CODGEO", all.x = TRUE, all.y = TRUE)

# Toutes les variables en pourcentages qui sont des variables budgétaires
variables_budg_prct <- colnames(df2011[which(str_detect(colnames(df2011), "_prct") == TRUE)])
variables_budg_prct <- variables_budg_prct[variables_budg_prct!= "geometry"] # On supprime la variable "geometry" qui est intégrée sans que cela soit voulu


# Suppression des données inutiles (déjà importées ou seulement de transition)
rm(df_budgets_2011, colbudgets2011, colgarder2011)
```

## Données électorales 2012

```{r import_data_electorales_2012, eval=TRUE}
PR2012_T1 <- read.table("data/elections/PR2012_T1.csv", sep="\t", colClasses = c(rep("character", 2), rep("numeric", 40)), head = TRUE, stringsAsFactors = TRUE, dec =",")
colnames(PR2012_T1)[3:42] <- paste0("PR2012_T1_", colnames(PR2012_T1)[3:42])

# PR2012_T2 <- read.table("data/elections/PR2012_T2.csv", sep="\t", colClasses = c(rep("character", 2), rep("numeric", 16)), head = TRUE, stringsAsFactors = TRUE, dec =",")

variables_elect <- c("PR2012_T1_Abst_prct_insc", "PR2012_T1_prct_insc_LE.PEN", "PR2012_T1_prct_insc_SARKOZY", "PR2012_T1_prct_insc_MÉLENCHON", "PR2012_T1_prct_insc_BAYROU", "PR2012_T1_prct_insc_HOLLANDE")

colPR2012_T1 <- colnames(PR2012_T1)
# colPR2012_T2 <- colnames(PR2012_T2)

# On liste les variables présentes seulement dans budgets (celles qu'il va falloir y prélever)
colgarder2011_PR2012_T1 <- setdiff(colPR2012_T1, coldf2011)
# colgarder2011_PR2012_T2 <- setdiff(colPR2012_T2, coldf2011)

# On fusionne les données de data_prep et des budgets, en ne gardant pour ces dernières que les données précédemment identifiées
df2011 <- merge(df2011,
                     PR2012_T1[, c("CODGEO", colgarder2011_PR2012_T1)],
                     by = "CODGEO", all.x = TRUE, all.y = TRUE)
# Suppression des données inutiles (déjà importées ou seulement de transition)
# df2011 <- merge(df2011,
#                      PR2012_T2[, c("CODGEO", colgarder2011_PR2012_T2)],
#                      by = "CODGEO", all.x = TRUE, all.y = TRUE)
rm(PR2012_T1, colPR2012_T1, coldf2011, colgarder2011_PR2012_T1)

```

## Données sur les RPI

Données pas encore mures (seulement données à la géométrie 2024).

```{r import donnees rpi, eval=FALSE}

load("data/RPI_par_communes_2024.Rdata")
df2011 <- merge(df2011, RPI_par_communes,
                     by = "CODGEO", all.x = TRUE)
table(df2011$RPI)
summary(is.na(df2011$RPI))
df2011$RPI[is.na(df2011$RPI)] <- "Sans_ecoles" # On indique comme étant "Sans_ecoles" les communes pour lesquelles aucune école n'était renseignée dans l'annuaire de l'Éducation nationale.
table(df2011$RPI)
summary(is.na(df2011$RPI))

```


## Données sur la BPE

```{r import data BPE}

load("data/BPE_2014.Rdata")
# On remplace l'absence de valeurs par des zéros
# BPE_communes[is.na(BPE_communes)] <- 0
rm(BPE_communes)
BPE_communes_decode[is.na(BPE_communes_decode)] <- 0

# Pour alléger les donées, on se concentre sur quelques équipements uniquements
BPE_communes_decode <- BPE_communes_decode[, c("CODGEO", "Coiffure", "Supermarché", "École élémentaire", "Médecin omnipraticien", "Taxi", "Salles multisports (gymnase)", "Infirmier", "Pharmacie")]

# Il faut ensuite recoder les variables car on veut faire des catégories
equipement <- "Pharmacie"
for (equipement in colnames(BPE_communes_decode)[2:ncol(BPE_communes_decode)]) {
  seuils <- quantile(BPE_communes_decode[, equipement], probs = seq(0, 1, 0.05), na.rm = TRUE) # Définition des seuils par déciles
  seuils <- unique(seuils) # On ne prend que des valeurs uniques
  seuils[length(seuils)] <- seuils[length(seuils)]+1 # On rajoute 1 à la dernière valeur pour que les catégories définies ensuite englobent toutes les communes
  BPE_communes_decode[, paste0(equipement, "_quantile")] <- cut(BPE_communes_decode[, equipement], breaks = seuils, right = FALSE) # right : pour ouverture de la coupure vers la gauche et non vers la droite comme par défait
  levels(BPE_communes_decode[, paste0(equipement, "_quantile")])
}

df2011 <- merge(df2011, BPE_communes_decode,
                     by = "CODGEO", all.x = TRUE)
```


# Préparation des données

## Sélection études de cas
Section qui sert au départ à tester le code sur de petits ensembles.

À l'avenir, pourra servir à définir des boucles

```{r definition_etudes_de_cas}
EdCs <- c("EdC_France", "EdC_Savoies", "EdC_Ouest", "EdC_Normandie", "EdC_49", "EdC_RALP_partiel")

df2011$EdC_France <- "OUI" # France entière

df2011$EdC_Savoies[df2011$CODE_DEPT %in% c("73", "74")] <- "OUI" # Départements de Savoie et Haute-Savoie
df2011[, c("EdC_Savoies")] [is.na(df2011[, c("EdC_Savoies")])] <- "NON"

df2011$EdC_Ouest[df2011$REG %in% c("23", "25", "53", "52")] <- "OUI" # Haute-Normandie, Basse Normandie, Bretagne, Pays-de-la-Loire
df2011[, c("EdC_Ouest")] [is.na(df2011[, c("EdC_Ouest")])] <- "NON"

df2011$EdC_Normandie[df2011$REG %in% c("23", "25")] <- "OUI" # Haute et Basse Normandie
df2011[, c("EdC_Normandie")] [is.na(df2011[, c("EdC_Normandie")])] <- "NON"

df2011$EdC_49[df2011$CODE_DEPT %in% c("49")] <- "OUI" # Département du Maine-et-Loire
df2011[, c("EdC_49")] [is.na(df2011[, c("EdC_49")])] <- "NON"

df2011$EdC_RALP_partiel[df2011$CODE_DEPT %in% c("69", "73", "74", "38", "01")] <- "OUI" # Départements de Savoie, Haute-Savoie, Isère, Rhône et Ain
df2011[, c("EdC_RALP_partiel")] [is.na(df2011[, c("EdC_RALP_partiel")])] <- "NON"

EdC <- EdCs[1]

df_reg <- subset(df2011, df2011[EdC] == "OUI")
df_reg_Cfus <- subset(df_reg, df_reg$COM_NOUV == "OUI")
# On peut choisir de se limiter aux communes françaises ayant une population ne dépassant pas celle de la plus peuplée des communes nouvelles
df_reg <- subset(df_reg, df_reg$P09_POP <= max(df_reg_Cfus$P09_POP))

# On ajoute les géométries
geomfus2011 <- merge(geomfus2011, df_reg[, c("CODGEO", "LIBGEO")], by = "CODGEO", all.x = FALSE, all.y = TRUE)
df_reg <- merge(geom2011, df_reg, by = "CODGEO", all.y = TRUE)



plot(df_reg$geom, main = paste("Espace étudié :", EdC))
```

## Adaptation variables pourcentages

NB : Il y a un problème si on utilise les variables budgétaires en valeurs absolues ou les variables en pourcentages notées 100% = 100. Cela fonctionne correctement si on a les pourcentages notées en 100% = 1.

Deux possibilités : diviser les pourcentages manuellement ou utiliser le fichier 'refdata_budgets_prct_en_ratio.Rdata'. On choisit la première solution.

Pas vraiment nécessaire car passage des variables quantitatives en variables qualitatives (quantiles).

```{r data_prct_RT, eval=FALSE}


# Pour multiplier/diviser
# Toutes les variables en pourcentages qui sont des variables budgétaires
variables_budg_prct <- colnames(df_reg[which(str_detect(colnames(df_reg), "_prct") == TRUE)])
variables_budg_prct <- variables_budg_prct[variables_budg_prct!= "geometry"] # On supprime la variable "geometry" qui est intégrée sans que cela soit voulu
df_reg [variables_budg_prct] <- df_reg[variables_budg_prct]/100
# test <- df_reg[variables_budg_prct]

variables_RT <- colnames(df_reg[which(str_detect(colnames(df_reg), "_RT") == TRUE)])
variables_RT <- variables_RT[variables_RT!= "geometry"] # On supprime la variable "geometry" qui est intégrée sans que cela soit voulu
df_reg [variables_RT] <- df_reg[variables_RT]/100
# test <- df_reg[variables_RT]

# Sans que je ne comprenne bien pourquoi, les géométries sont également touchées, si variable non retirée avant, on rétablit donc ces dernières
# df_reg$geometry <- df_reg$geometry * 10000

```


## Transformations variables quantitatives en variables qualitatives

Le principe d'une régression logistique est de définir la probabilité d'un évènement (dans mon cas, fusionner ou non) en fonction du changement d'une variable. Dans le cas d'une variable quantitative, la question est quelle est le pourcentage de chance que l'évènement survienne davantage lorsqu'on augmente la variable quantitative de 1.

Dans le cas d'une variable qualitative, il s'agit du pourcentage de chance que l'évènement survienne davantage lorsqu'on change de catégorie. Il faut donc faire attention à mettre la variable en facteur, avec la première valeur qui doit être celle de référence (c'est à partir de celle-là que les comparaisons vont être faites, c'est donc important car c'est cela qui détermine le discours). 


```{r modfi_quanti_quali, eval=TRUE}

# variable <- "C09_ACTOCC_OUT_RT"
variables_quanti_quali <- c(variables_budg_prct, variables_RT, variables_elect)
# On retire certaines variables qui ne peuvent être divisées par tertiles car trop de valeurs nulles.
variables_quanti_quali <- variables_quanti_quali[variables_quanti_quali!="DSU_tot_prct"]
variables_quanti_quali <- variables_quanti_quali[variables_quanti_quali!="DNP_princip_prct"]
variables_quanti_quali <- variables_quanti_quali[variables_quanti_quali!="DNP_sortie_prct"]
variables_quanti_quali <- variables_quanti_quali[variables_quanti_quali!="DNP_major_prct"]
variables_quanti_quali <- variables_quanti_quali[variables_quanti_quali!="DNP_tot_prct"]
variables_quanti_quali <- variables_quanti_quali[variables_quanti_quali!="DSR_bourg_centre_global_prct"]
variables_quanti_quali <- variables_quanti_quali[variables_quanti_quali!="DSR_cible_global_prct"]
variables_quanti_quali <- variables_quanti_quali[variables_quanti_quali!="C09_EMPLT_INDUS_RT"]

df_sans_geom <- df_reg # Création d'un jeu de données pour le travail
st_geometry(df_sans_geom) <- NULL # Suppression des géométries
# summary(df_sans_geom[, variable])
variable <- "superficie"


for (variable in variables_quanti_quali) {

Y <- df_sans_geom[, variable] # On se focalise sur une variable en particulier

# Calcul moyenne groupes
moy <- as.data.frame(tapply(df_sans_geom[, variable], df_sans_geom$COM_NOUV, mean, na.rm = TRUE))
moy$COM_NOUV <- row.names(moy)
colnames(moy) <- c("moyenne", "COM_NOUV") 

# Calcul moyenne groupes
med <- as.data.frame(tapply(df_sans_geom[, variable], df_sans_geom$COM_NOUV, median, na.rm = TRUE))
med$COM_NOUV <- row.names(moy)
colnames(med) <- c("median", "COM_NOUV") 

# On veut observer graphiquement la distribution statistique
histo <- ggplot(df_sans_geom)+
  geom_histogram(aes(x=df_sans_geom[, variable], color = COM_NOUV), bins=50)+
  labs(title = paste("Variable étudiée :\n", variable), subtitle = "Pointillés : moyennes.\nLigne pleine : médianes.")+
  # geom_vline(aes(xintercept=median(df_sans_geom[, variable], na.rm = T)),      color="black", linetype="dashed" )+
  geom_vline(data=moy, aes(xintercept=moyenne, color=COM_NOUV),             linetype="dashed") +
  geom_vline(data=med, aes(xintercept=median, color=COM_NOUV)) +
  ylab("Count")+theme_light()
print(histo)

# On découpe en fonction du quantile souhaité puis on modifie l'ordre de niveaux de facteur pour que la variable centrale soit la première

coupures <- quantile(Y, probs=seq(0, 1, 0.2), na.rm = TRUE)

if (any(duplicated(coupures)) == FALSE) { # Si c'est possible, on utilise les quintiles
  Y <- cut(Y,breaks=c(quantile(Y, probs=seq(0, 1, 0.2), na.rm = TRUE))) # Pour découpage en quintiles
levels(Y)<-c("Q1","Q2", "Q3", "Q4", "Q5")
Y <- relevel (Y, "Q3","Q1", "Q2", "Q4", "Q5")
}else{# Sinon, on utilise les tertiles
Y <- cut(Y,breaks=c(quantile(Y, probs=seq(0, 1, 1/3), na.rm = TRUE))) # Pour découpage en tertiles
levels(Y)<-c("Tertile_inf","Tertile_med", "Tertile_sup")
Y <- relevel (Y, "Tertile_med", "Tertile_inf", "Tertile_sup")
}


# Y<-cut(Y,breaks=c(quantile(Y))) # Pour découpage en quartiles
# levels(Y)<-c("Q1","Q2", "Q3", "Q4")




summary(Y)


X <- df_sans_geom$COM_NOUV
summary(X)

tabcont<-table(X,Y)
tabcont # En valeur absolue
# round(100*prop.table(tabcont,margin=1),1) # Pourcentages, le total se fait par lignes
# round(100*prop.table(tabcont,margin=),1) # Pourcentages, le total se fait sur l'ensemble de la population
# round(100*prop.table(tabcont,margin=2),1) # Pourcentages, le total se fait par colonnes

# On verse les nouvelles données au data frame
df_reg[, paste0(variable, "_quali")] <- Y

}



rm(tabcont, X, Y, df_sans_geom, variable, histo)
```

## Modifications de quelques autres variables


```{r prep_variables_ZAU_etc}

# On réordonne le facteur CATAEU2010
df_reg$CATAEU2010 <- as.factor(df_reg$CATAEU2010)
freq(df_reg$CATAEU2010, sort = "dec")
row.names(freq(df_reg$CATAEU2010, sort = "dec"))
# Vérifier que la modalité qui se trouve en premier est bien la plus fréquente.
# Sinon, possibilité d'utiliser le code suivant :
# df_reg$CATAEU2010 <- relevel (df_reg$CATAEU2010, "112", "111", "120", "211", "212", "221", "222", "300", "400")
# df_reg$CATAEU2010 <- relevel (df_reg$CATAEU2010, "112", "400", "300", "120", "111", "221", "212", "222", "211")
ordre <- row.names(freq(df_reg$CATAEU2010, sort = "dec"))
df_reg$CATAEU2010 <- factor (df_reg$CATAEU2010, ordre)


df_reg$CODE_DEPT <- as.factor(df_reg$CODE_DEPT)
freq(df_reg$CODE_DEPT, sort = "dec")
ordre <- row.names(freq(df_reg$CODE_DEPT, sort = "dec"))
df_reg$CODE_DEPT <- factor (df_reg$CODE_DEPT, ordre)

df_reg$REG <- as.factor(df_reg$REG)
freq(df_reg$REG, sort = "dec")
row.names(freq(df_reg$REG, sort = "dec"))
ordre <- row.names(freq(df_reg$REG, sort = "dec"))
df_reg$REG <- factor (df_reg$REG, ordre)


df_reg$P09_POP <- as.numeric(df_reg$P09_POP)
df_reg$P09_POP_quali <- cut(df_reg$P09_POP, breaks=c(quantile(df_reg$P09_POP, probs=seq(0, 1, 1/5), na.rm = TRUE)))
levels(df_reg$P09_POP_quali)<-c("Q1","Q2", "Q3", "Q4", "Q5")
df_reg$P09_POP_quali <- relevel (df_reg$P09_POP_quali, "Q3","Q1", "Q2", "Q4", "Q5")

df_reg$RPI <- as.factor(df_reg$RPI)
freq(df_reg$RPI, sort = "dec")
row.names(freq(df_reg$RPI, sort = "dec"))
ordre <- row.names(freq(df_reg$RPI, sort = "dec"))
ordre <- c("Ecoles_sans_RPI", "Sans_ecoles", "RPI", "Avec_et_sans_RPI")
df_reg$RPI <- factor (df_reg$RPI, ordre)


```


# Étude auto-corrélation et colinéarité

Pour éviter la colinéarité entre les variables explicatives, deux méthodes [feuillet2019] :

- faire une matrice de corrélation et supprimer les variables dont les corrélations sont supérieures à 0,7 en valeur absolue [tenenhaus2007] ;

- calculer la Variance Inflation Factor et supprimer les variables avec une VIF supérieure à 3 (voire supérieur à 2, même référence biblio).

## Réalisation d'une matrice de corrélation

Cf. https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html

```{r analyse_correlations_prep_data}
# Pour sélectionner certaines variables spécifiquement
# variables_correlations <- c("fctva_prct", "dgf_prct", "DSU_tot_prct", "DNP_tot_prct", "DSR_pereq_global_prct")
# variables_correlations <- c("fctva", "dgf", "DSU_tot", "DNP_tot", "DSR_pereq_global")
# variables_correlations <- vartests

# Toutes les variables en pourcentages NB : ajout du symbole $ pour indiquer que c'est la fin de la chaîne de caractère
variables_correlations <- c(colnames(df_reg[which(str_detect(colnames(df_reg), "_prct$") == TRUE)]),
                            colnames(df_reg[which(str_detect(colnames(df_reg), "_RT$") == TRUE)]))
variables_correlations <- c(colnames(df_reg[which(str_detect(colnames(df_reg), "_prct$") == TRUE)]),
                            colnames(df_reg[which(str_detect(colnames(df_reg), "_RT$") == TRUE)]), variables_elect)

variables_correlations <- variables_correlations[variables_correlations!= "geometry"] # On supprime la variable "geometry" si elle est intégrée sans que cela soit voulu

# variables_correlations <- c("P09_RETR1564_RT", "C09_ACT1564_Agr_RT", "P09_POP3044Y_RT", "C09_ACTOCC_OUT_RT", "P09_CHOM1564_RT", "dgf_prct", "perso_prct", "charge_prct")

# Définition du seuil d'auto-corrélation
seuil <- 0.7
```


```{r analyse_correlations_subsets}
# Juste pour les communes fusionnées
tmp <- subset(df2011, df2011$COM_NOUV == "OUI")
tmp <- na.omit(tmp [, variables_correlations])

tmp2 <- cor(tmp, method = "pearson")
res1 <- cor.mtest(tmp, conf.level = .99)

col <- c("#a50026","#d73027","#f46d43","#fdae61","#fee090","#e0f3f8","#abd9e9","#74add1","#4575b4","#313695")

corrplot::corrplot(tmp2, p.mat = res1$p, sig.level = .2, order = "hclust", tl.cex = 0.8, tl.col = "black") # Obligé d'appeler le package directement car sinon conflit avec une autre fonction du package `arm`.


corrplot::corrplot(tmp2, method="color", order = "hclust", # cl.lim = c(-1,1), # cl.lim pose problème donc supprimé
         col = col,
         tl.col = rgb(0,0,0),
         number.cex = 0.4,
         addCoef.col = rgb(0,0,0, alpha =0.6),
         addgrid.col = "white",
         title = "Analyse des corrélations",
         tl.cex=0.8,
         cl.cex = 0.6,
         mar=c(0,0,4,0))

tmp2[which(tmp2 == 1)] <- 0 # On remplace par 0 l'autocorrélations des variables avec elles-mêmes pour faciliter la lecture ensuite
# which(abs(tmp2) > 0.7, arr.ind=TRUE) # On identifie les valeurs dont la valeur absolue serait supérieure à 0,7

for(i in 1:nrow(tmp2)) {
   for(j in 1:ncol(tmp2))
   {if(tmp2[i,j]>seuil)
   {print(paste("Variables auto-corrélées à plus de", seuil, ":", rownames(tmp2)[i], "et", colnames(tmp2)[j], ":", tmp2[i,j]))
     }}}



# Communes n'ayant pas fusionné
tmp <- subset(df2011, df2011$COM_NOUV == "NON")
tmp <- na.omit(tmp [, variables_correlations])

tmp2 <- cor(tmp, method = "pearson")
res1 <- cor.mtest(tmp, method = "pearson")

corrplot::corrplot(tmp2, p.mat = res1$p, sig.level = .2, order = "hclust",
         tl.cex = 0.8, tl.col = "black")

col <- c("#a50026","#d73027","#f46d43","#fdae61","#fee090","#e0f3f8","#abd9e9","#74add1","#4575b4","#313695")

corrplot::corrplot(tmp2, method="color", order = "hclust", # cl.lim = c(-1,1), # cl.lim pose problème donc supprimé 
         col = col,
         tl.col = rgb(0,0,0),
         number.cex = 0.4,
         addCoef.col = rgb(0,0,0, alpha =0.6),
         addgrid.col = "white",
         title = "Analyse des corrélations",
         tl.cex=0.8,
         cl.cex = 0.6,
         mar=c(0,0,4,0))

tmp2[which(tmp2 == 1)] <- 0 # On remplace par 0 l'autocorrélations des variables avec elles-mêmes pour faciliter la lecture ensuite
# which(abs(tmp2) > 0.7, arr.ind=TRUE) # On identifie les valeurs dont la valeur absolue serait supérieure à 0,7

for(i in 1:nrow(tmp2)) {
   for(j in 1:ncol(tmp2))
   {if(tmp2[i,j]>seuil)
   {print(paste("Variables auto-corrélées à plus de", seuil, ":", rownames(tmp2)[i], "et", colnames(tmp2)[j], ":", tmp2[i,j]))
     }}}
```


```{r analyse_correlations_ttes_communes}
# Ensemble des communes françaises
tmp <- df2011
tmp <- na.omit(tmp [, variables_correlations])

tmp2 <- cor(tmp, method = "pearson")
res1 <- cor.mtest(tmp, method = "pearson")

corrplot::corrplot(tmp2, p.mat = res1$p, sig.level = .2, order = "hclust",
         tl.cex = 0.8, tl.col = "black")

col <- c("#a50026","#d73027","#f46d43","#fdae61","#fee090","#e0f3f8","#abd9e9","#74add1","#4575b4","#313695")

corrplot::corrplot(tmp2, method="color", order = "hclust", # cl.lim = c(-1,1), # cl.lim pose problème donc supprimé 
         col = col,
         tl.col = rgb(0,0,0),
         number.cex = 0.4,
         addCoef.col = rgb(0,0,0, alpha =0.6),
         addgrid.col = "white",
         title = "Analyse des corrélations",
         tl.cex=0.8,
         cl.cex = 0.6,
         mar=c(0,0,4,0))

tmp2[which(tmp2 == 1)] <- 0 # On remplace par 0 l'autocorrélations des variables avec elles-mêmes pour faciliter la lecture ensuite
# which(abs(tmp2) > 0.7, arr.ind=TRUE) # On identifie les valeurs dont la valeur absolue serait supérieure à 0,7

for(i in 1:nrow(tmp2)) {
   for(j in 1:ncol(tmp2))
   {if(tmp2[i,j]>seuil)
   {print(paste("Variables auto-corrélées à plus de", seuil, ":", rownames(tmp2)[i], "et", colnames(tmp2)[j], ":", tmp2[i,j]))
     }}}


```

Les variables envisagées ne sont pas trop auto-corrélées entre elles si on se réfère aux seuils préconisés dans la littérature.

Si jamais des variables sont trop autocorrélées, on peut les supprimer (c'est ce qui est fait dans la section suivante) puis relancer une étude de l'auto-corrélation.

```{r suppr_variables}
variables_correlations <- variables_correlations[variables_correlations!="impo1_prct"]
variables_correlations <- variables_correlations[variables_correlations!="Pop_INSEE_prct"]
variables_correlations <- variables_correlations[variables_correlations!="Pop_DGF_prct"]
variables_correlations <- variables_correlations[variables_correlations!="Dotation_forfaitaireN_prct"]
variables_correlations <- variables_correlations[variables_correlations!="pfb_prct"]
variables_correlations <- variables_correlations[variables_correlations!="DNP_princip_prct"]
variables_correlations <- variables_correlations[variables_correlations!="DNP_major_prct"]
variables_correlations <- variables_correlations[variables_correlations!="DNP_tot_prct"]
variables_correlations <- variables_correlations[variables_correlations!="DSR_pereq_global_prct"]
variables_correlations <- variables_correlations[variables_correlations!="P11_POT_FIN_RT"]
variables_correlations <- variables_correlations[variables_correlations!="P11_Rev_Fisc_RT"]
variables_correlations <- variables_correlations[variables_correlations!="P11_IMP_NET_RT"]
variables_correlations <- variables_correlations[variables_correlations!="res1_prct"]
variables_correlations <- variables_correlations[variables_correlations!="depinv_prct"]
variables_correlations <- variables_correlations[variables_correlations!="tth_prct"]
variables_correlations <- variables_correlations[variables_correlations!="prod_prct"]
variables_correlations <- variables_correlations[variables_correlations!="res2_prct"]
variables_correlations <- variables_correlations[variables_correlations!="remb_prct"]
variables_correlations <- variables_correlations[variables_correlations!="pfnb_prct"]




```

```{r suppr_data_autocorr}
rm(tmp, tmp2, res1, col, i, j, seuil)

```



## Calcul de la *Variance Inflaction Factor* (VIF)

D'après la littérature [tenenhaus2007] , il faut calculer la Variance Inflation Factor (VIF) et supprimer les variables avec une VIF supérieure à 3 (voire supérieur à 2).

Pour la mise en œuvre, cf. cette page : https://larmarange.github.io/guide-R/analyses/multicolinearite.html

Ces éléments seront donc présentés lors de la mise en œuvre de la régression.

# Comparaison des valeurs des communes fusionnantes vis-à-vis des valeurs des communes inchangées

```{r comparaisons_valeurs_moyennes}

# Calculs des valeurs moyennes (pour comparaison)
# On identifie les variables en stock
variables_stock <- stringr::str_replace(variables_RT, "Y_RT", "")
variables_stock <- stringr::str_replace(variables_stock, "_RT", "")
variables_stock <- variables_stock[-which(variables_stock == "C09_EMP_CONC")]
variables_stock <- variables_stock[1:(length(variables_stock)-5)]

# Import données totales
# load("data/refdata.Rdata")
# rm(df_new)
# Sélection données utiles
pourmoyennesCAH <- na.omit(subset(df2011, COM_NOUV == "OUI"))
moyennesCAH <- data.frame()
i <- variables_stock[2]
ratio <- as.data.frame(read_excel("data-raw/meta.xlsx", sheet = "ratios"))
row.names(ratio) <- ratio$Numerator_Code

for (i in variables_stock){
  a <- ratio[i, "Denominator_Code"]
  b <- ratio[i, "Coeff"]
  c <- sum(df2011[, i], na.rm = TRUE)
  d <- sum(df2011[, a], na.rm = TRUE)
  moyfr <- round(b * c/d, 2)
  e <- sum(pourmoyennesCAH[, i], na.rm = TRUE)
  f <- sum(pourmoyennesCAH[, a], na.rm = TRUE)
  moyCFus <- round(b * e/f, 2)
  g <- ratio[i, "CODE"]
  h <- c(g, moyfr, moyCFus)
  moyennesCAH <- rbind(moyennesCAH, h, stringsAsFactors= FALSE)
  rm( a, b, c, d, e, f, g, h, moyfr, moyCFus)
}

colnames(moyennesCAH) <- c("Variable", "France", "CommunesFusionnantes")
moyennesCAH$Variable <- as.factor(moyennesCAH$Variable)
moyennesCAH$France <- as.numeric(moyennesCAH$France)
moyennesCAH$CommunesFusionnantes <- as.numeric(moyennesCAH$CommunesFusionnantes)
moyennesCAH$DiffComFusComFr <- moyennesCAH$CommunesFusionnantes - moyennesCAH$France

aa<- ggplot(data = moyennesCAH) +
  geom_bar(aes(x = Variable, y = France), stat = "identity") + coord_flip()
bb <- ggplot(data = moyennesCAH) +
  geom_bar(aes(x = Variable, y = CommunesFusionnantes), stat = "identity") + coord_flip()
cc <- ggplot(data = moyennesCAH) +
  geom_bar(aes(x = Variable, y = DiffComFusComFr), stat = "identity") + coord_flip() + ylab(paste0("Différence Communes fusionnantes - ", "France"))
cowplot::plot_grid(aa, bb, cc, ncol = 1, nrow = 3) 

# Pour variables politiques
# Calculs des valeurs moyennes (pour comparaison)
# On identifie les variables en stock
VarCAHBrutes <- stringr::str_replace(variables_elect, "prct_insc", "nbre_voix")
# Quelques cas particuliers
VarCAHBrutes <- stringr::str_replace(VarCAHBrutes, "Abst_nbre_voix", "Abst_nbre")
VarCAHBrutes <- stringr::str_replace(VarCAHBrutes, "BlcsNuls_nbre_voix", "BlcsNuls_nbre")

elect <- substr(VarCAHBrutes[1], start = 1, stop = 9)

# Sélection données utiles
pourmoyennesCAH <- na.omit(subset(df2011, COM_NOUV == "OUI"))
somme_inscr_CFus <- sum(pourmoyennesCAH[, paste0(elect, "_Inscr_nbre")], na.rm = TRUE)
somme_inscr_ttes_com <- sum(na.omit(df2011[, paste0(elect, "_Inscr_nbre")]))

moyennesCAH <- data.frame()

i <- VarCAHBrutes[3]

for (i in VarCAHBrutes){
  somme_ttes_com <- sum(df2011[, i], na.rm = TRUE)
  moy_ttes_com <- round(100*somme_ttes_com/somme_inscr_ttes_com, 2)
  somme_CFus <- sum(pourmoyennesCAH[, i], na.rm = TRUE)
  moy_Cfus <- round(100*somme_CFus/somme_inscr_CFus, 2)
  variable <- variables_elect[ which(VarCAHBrutes == i) ]
  ligne <- c(variable, moy_ttes_com, moy_Cfus)
  moyennesCAH <- rbind(moyennesCAH, ligne, stringsAsFactors= FALSE)
  
}
rm (somme_inscr_CFus, somme_inscr_ttes_com, somme_ttes_com, moy_ttes_com, somme_CFus, moy_Cfus, ligne, variable)

colnames(moyennesCAH) <- c("Variable", "Ensemble_étudié", "CommunesFusionnantes")
moyennesCAH$Variable <- as.factor(moyennesCAH$Variable)
moyennesCAH$Ensemble_étudié <- as.numeric(moyennesCAH$Ensemble_étudié)
moyennesCAH$CommunesFusionnantes <- as.numeric(moyennesCAH$CommunesFusionnantes)
moyennesCAH$DiffComFusComFr <- moyennesCAH$CommunesFusionnantes - moyennesCAH$Ensemble_étudié

aa<- ggplot(data = moyennesCAH) +
  geom_bar(aes(x = Variable, y = Ensemble_étudié), stat = "identity") + coord_flip() + ylab("France")
bb <- ggplot(data = moyennesCAH) +
  geom_bar(aes(x = Variable, y = CommunesFusionnantes), stat = "identity") + coord_flip() + ylab("Communes fusionnantes")
cc <- ggplot(data = moyennesCAH) +
  geom_bar(aes(x = Variable, y = DiffComFusComFr), stat = "identity") + coord_flip() + ylab(paste0("Différence Communes fusionnantes - ", "France"))
cowplot::plot_grid(aa, bb, cc, ncol = 1, nrow = 3)



```



# Régression logistique classique

Cf. par exemple https://larmarange.github.io/analyse-R/regression-logistique.html.

On fait le choix d'analyser ces données sous l'angle d'une régression logistique. Il s'agit d'observer si des variables augmentent ou diminuent la probabilité d'avoir une situation (fusion ou non). (Les tests de régression logistique sont utilisées parfois pour observer si des caractéristiques, qui seraient donc alors des comorbidités, augmentent ou non la probabilité d'avoir un décès.)


## Variables explicatives envisagées

Les variables explicatives envisagées sont celles qui ont été pointées comme caractérisant davantage les communes nouvelles lors des études univariées, bivariées, ou lors de catégorisations [@bideau2019, @bideau2022c].

Le choix des variables explicatives envisagées est déterminant, comme cela a pu être montré par exemple par @afsa2016. En effet, le modèle logit présente la probabilité d'estimer la probabilité qu'un évènement se produise (dans notre cas, fusion ou non) mais il seulement en prenant en compte les données présentes dans le modèle. Dans le cas présenté par @afsa2016, si on n'intègre pas les CSP des parents, le fait d'être en ZEP a un impact très négatif sur l'orientation en seconde. Si on intègre ces CSP, la significativité de la ZEP disparaît presque intégralement, si on ajoute le niveau en 6e, le fait d'être en ZEP redevient significatif mais un facteur favorable pour cette orientation.


```{r variables_expl}
# Si l'on souhaite avoir des noms de variables plus explicites, il faut ajouter des étiquettes des variables avec var_label de l'extension labelled. Par exemple :
# var_label(d$sport) <- "Pratique du sport ?"

```

## Modalités de la variable d'intérêt

Il est d'abord pertinent de vérifier que ce qui va être perçu, dans notre variable d'intérêt (ici la fusion ou non) comme la modalité de référence est bien la situation normale, la plus fréquente. Dans notre cas, la très grandes majorité des communes françaises n'a pas connu de fusion, c'est donc la non-fusion qui est notre modalité de référence.


```{r reg_logist_variable_interet}

# Nécessité de passer la variable d'intérêt en facteur pour la fonction glm suivante
df_reg$COM_NOUV <- as.factor(df_reg$COM_NOUV)
class(df_reg$COM_NOUV)
freq(df_reg$COM_NOUV)

# Vérification que la variable la plus fréquente est bien la première (doit être la variable de référence)
freq(df_reg$COM_NOUV, sort = "dec")
row.names(freq(df_reg$COM_NOUV, sort = "dec"))
ordre <- row.names(freq(df_reg$COM_NOUV, sort = "dec"))
df_reg$COM_NOUV <- factor (df_reg$COM_NOUV, ordre)

# Si on veut forcer dans un sens :
# df_reg$COM_NOUV <- relevel (df_reg$COM_NOUV, "NON", "OUI")

df_reg$ChefLieu <- as.factor(df_reg$ChefLieu)
freq(df_reg$ChefLieu, sort = "dec")
row.names(freq(df_reg$ChefLieu, sort = "dec"))
ordre <- row.names(freq(df_reg$ChefLieu, sort = "dec"))
df_reg$ChefLieu <- factor (df_reg$ChefLieu, ordre)


```


## Mise en œuvre de la régression logistique


Choix des variables qui doivent éviter celles pointées comme auto-corrélées plus haut. On affiche également le VIF pour limiter au maximum les biais d'interprétation.


<!-- L’approche la plus classique consiste à examiner les facteurs d’inflation de la variance (FIV) ou variance inflation factor (VIF) en anglais. Les FIV estimenent de combien la variance d’un coefficient est augmentée en raison d’une relation linéaire avec d’autres prédicteurs. Ainsi, un FIV de 1,8 nous dit que la variance de ce coefficient particulier est supérieure de 80 % à la variance que l’on aurait dû observer si ce facteur n’est absolument pas corrélé aux autres prédicteurs. -->

<!-- Si tous les FIV sont égaux à 1, il n’existe pas de multicolinéarité, mais si certains FIV sont supérieurs à 1, les prédicteurs sont corrélés. Il n’y a pas de consensus sur la valeur au-delà de laquelle on doit considérer qu’il y a multicolinéarité. Certains auteurs, comme Paul Allison, disent regarder plus en détail les variables avec un FIV supérieur à 2,5. D’autres ne s’inquiètent qu’à partir de 5. Il n’existe pas de test statistique qui permettrait de dire s’il y a colinéarité ou non. (https://larmarange.github.io/analyse-R/multicolinearite.html)-->

```{r regression_logistique, warning=FALSE}




# Les variables explicatives sont mentionnées juste après la variable d'intérêt (ici, "COM_NOUV")
reg <- glm(COM_NOUV ~ 
# reg <- glm(ChefLieu ~ 
             # dgf_prct + perso_prct + equip_prct + dette_prct,
             # dgf_prct + perso_prct + equip_prct,
             # P09_RETR1564_RT + C09_ACT1564_Agr_RT + P09_POP3044Y_RT + C09_ACTOCC_OUT_RT + P09_CHOM1564_RT + dgf_prct + perso_prct + charge_prct,
             # P09_RETR1564_RT + C09_ACT1564_Agr_RT + P09_POP3044Y_RT + C09_ACTOCC_OUT_RT + P09_CHOM1564_RT + dgf_prct + perso_prct,
             # P09_RETR1564_RT + C09_ACT1564_Agr_RT + C09_ACTOCC_OUT_RT + P09_CHOM1564_RT,
             # P09_ETUD1564_RT + C09_ACT1564_Ouvr_RT + C09_ACTOCC_OUT_RT + P09_CHOM1564_RT,
             # C09_ACTOCC_OUT_RT_quali + P09_CHOM1564_RT + dgf_prct,
             # C09_ACTOCC_OUT_RT_quali + P09_CHOM1564_RT_quali + dgf_prct_quali, # Effets faibles mais significatifs
             # P09_ETUD1564_RT_quali + C09_ACT1564_Ouvr_RT_quali + C09_ACTOCC_OUT_RT_quali + P09_CHOM1564_RT_quali,
             # C09_ACTOCC_OUT_RT_quali,
             # C09_ACTOCC_OUT_RT_quali + P09_CHOM1564_RT_quali + P09_POP1529Y_RT_quali + P09_POP_quali + PR2012_T1_Abst_prct_insc_quali + PR2012_T1_prct_insc_LE.PEN_quali + CATAEU2010 + CODE_DEPT,
             # P09_ETUD1564_RT_quali + C09_ACT1564_Ouvr_RT_quali + C09_ACTOCC_OUT_RT_quali + P09_CHOM1564_RT_quali + P09_POP1529Y_RT_quali + P09_POP_quali + PR2012_T1_Abst_prct_insc_quali + PR2012_T1_prct_insc_LE.PEN_quali + dgf_prct_quali + perso_prct_quali + CATAEU2010,
             # P09_ETUD1564_RT_quali + C09_ACT1564_Ouvr_RT_quali + C09_ACTOCC_OUT_RT_quali + P09_CHOM1564_RT_quali + P09_POP1529Y_RT_quali + P09_POP_quali + PR2012_T1_Abst_prct_insc_quali + PR2012_T1_prct_insc_LE.PEN_quali + dgf_prct_quali + perso_prct_quali + CATAEU2010 + superficie,
              # CATAEU2010 + P09_POP_quali,
              # CATAEU2010 + RPI,
              # CATAEU2010 + `École élémentaire_quantile`,
              # CATAEU2010 + `École élémentaire_quantile` + `Médecin omnipraticien_quantile`,
              CATAEU2010 + `Médecin omnipraticien_quantile`,

           data = df_reg, family = binomial(logit), na.action = na.exclude)

summary(reg)
# exp(coef(reg))
# exp(confint(reg))
# exp(cbind(coef(reg), confint(reg)))
# odds.ratio(reg)

# Pour vérifier les VIF
car::vif(reg)
which(car::vif(reg) > 1.9)

# Présentation d'un tableau plus propre
# tbl_regression(reg, exponentiate = TRUE)
add_vif(tbl_regression(reg, exponentiate = TRUE)) # Avec affichage des VIF

# Représentations graphiques du modèle
# ggcoef_model(reg, exponentiate = FALSE)
forest_model(reg)

# Représentation graphique des effets
# plot(allEffects(reg))

plot_grid(plotlist = plot(ggeffect(reg)))
# Pour n'afficher qu'une variable :
# plot(ggeffect(reg, "_nom_de_la_variable"))

screenreg(reg, stars = c(0.01, 0.05, 0.1))

# texreg(reg, stars = c(0.01, 0.05, 0.1)) # Pour export LaTeX


```

## [À vérifier] Lecture du modèle


Les odds ratio (littéralement rapport des cotes, ou rapport des chances ou rapport des risques relatiffs) permettent de décrire la significativité pratique. Celle-ci désigne l'impact d'une variable. Il ne faut pas la confondre avec la significativité statistique, qui décrit « le degré de certitude avec lequel on peut affirmer qu'une variable influe » (@afsa2016 p. 39).

À noter que, par exemple pour @afsa2016, l'expression « toutes choses égales par ailleurs » est à éviter. En effet, ce n'est le cas que si les variables étudiées permettent de décrire parfaitement le phénomène. « les résultats des estimations sont *conditionnels* à la liste des variables *x* introduites dans le modèle, c'est-à-dire qu’ils dépendent des variables introduites » (@afsa2016 p. 53). Si des variables, non mesurées, influent à la fois sur les variables du modèle et la variable étudiée, cela brouille l'analyse. Ainsi, pour les analyses présentées ici, c'est plutôt l'expression "Toutes choses égales quant aux autres variables introduites" qui est à utiliser.

Un Odds ratio égal à 1 signifie que la variable d'intérêt (la fusion ici) est indépendante de la variable explicative envisagée. Si l'OR est supérieur à 1, une commune ayant une variable explicative **plus élevée** ont **plus** de chance de fusionner, s'il est inférieur à 1, les communes avec une variable explicative **plus faible** ont **moins** de chance de fusionner.

```{r tempchunk, eval=FALSE, echo=FALSE}
# Formulation dont je ne suis pas du tout certain de la validitié.

paste("Toutes choses égales quant aux autres variables introduites, un ODDS RATIO de", exp(reg$coefficients[2]), "signifie que les communes ayant fusionné ont une probabilité de",  100 *(exp(reg$coefficients[2]) - 1), "%  d'avoir une DGF plus élevée que les communes n'ayant pas fusionné")


```


## Sauvegarde données modèle


```{r sauvegarde_modele}

try(save(df2011, df_new, df_reg, reg, file = "data/refdata_modele.Rdata"))

load("data/refdata_modele.Rdata")

```


# [À faire] Étude des résidus (y compris avec indice de Moran pour une auto-corrélation spatiale des résidus ?)

Non-stationnarité spatiale : lorsque les paramètres permettant d'expliquer un phénomènes sont différents en fonction des espaces. Bonne figure p. 185 (Feuillet et al, 2019) où en regardant deux groupes distincts on aperçoit un type de relation alors que lorsqu'on regarde les deux en même temps, on imagine une autre relation statistique. C'est le paradoxe de Simpson (qui n'est pas forcément situé).

Il est donc important voire intéressant de cartographier ou de représenter graphiquement les résidus. Cela permet potentiellement de valider ou invalider le modèle mais également de réfléchir aux espaces où ce modèle fonctionne le mieux ou le moins bien.

## Étude résidus régression linéaire (peu pertinent dans le cas d'un logit)

Cf. *R et Espace* section 5.2.3 et particulièrement p. 91 sq pour aperçu des résidus : « La fonction plot() appliquée aux résultats d’un modèle de régression linéaire obtenus par la fonction lm() permet de représenter les quatre principales hypothèses au cœur de ce modèle :

— la normalité des résidus par rapport aux valeurs prédites (en haut à gauche de l’image)

— la normalité globale des résidus (en haut à droite de l’image)

— la corrélation entre les valeurs de la variable explicative et le carré des résidus standardisés (en bas à gauche de l’image)

— l’existence de valeurs extrêmes altérant l’estimation des paramètres (en bas à droite de l’image) »

Autre source très utilisée ci-dessous : https://sites.google.com/site/rgraphiques/4--stat/machine-learning-biostatistiques-analyse-de-donn%C3%A9es/r%C3%A9gressions-lin%C3%A9aires-avec-r?pli=1

```{r etude_residus, eval=FALSE}


# On veut éviter les phénomènes d'auto-corrélation : les résidus augmenteraient ensemble dans une zone donnée et seraient liés les uns aux résidus précédents ou suivants (cas typiques lorsque X est un enregistrement du temps). Vérification par le test de Durbin-Watson (indépendance : p-value > 0,05)
durbinWatsonTest(reg)
dwtest(reg)

# Visualisation des résidus
par(mfrow = c(2,2))
plot(reg)
par(mfrow = c(1,1))

# Les résidus doivent être répartis approximativement selon une courbe de Gauss
hist(residuals(reg),col="yellow",freq=F)

# Il faut, pour que la régression soit pertinente, observer un alignement entre les quantiles des résidus et les quantiles théoriques, donc globalement une ligne droite
plot(reg, which = 2)
# On peut vérifier avec le test de normalité de Shapiro-Wilk (normalité si p-value supérieure à 0,05)
shapiro.test(residuals(reg))


# Vérification que la variance des résidus est constante (distribution homogène)
# Pour que le modèle soit pertinent, il faut une courbe rouge plane et que les valeurs ne se regroupent pas.
plot(reg, which = 3)
# On peut vérifier avec le test de Breush-Pagan (homogénéité si p-value > 0,05)
# ncvTest(reg) # seulement pour objet lm
bptest(reg)
# Ou test de Goldfeld-Quandt (homogénéité si p-value > 0,05)
gqtest(reg)

plot(reg, which = 5) 



```


## Étude résidus logit

Cf. https://pmarchand1.github.io/ECL7102/notes_cours/9-Regression_logistique.pdf#page=9

```{r etude_residus_logit, eval=FALSE}

binnedplot(fitted(reg), residuals(reg, type = "response"))


```


## Tentative cartographie résidus

```{r carto_residus}

# residuals (reg)
# On intègre les résidus aux données
df_reg$residus <- residuals (reg)
par(mar=c(0,0,1.2,0), mfrow = c(1,2))

plot(st_geometry(dep), col = NA)

choroLayer(x = df_reg , var = "residus", 
           method = "quantile", nclass = 6,
           col = carto.pal(pal1 = "blue.pal", pal2 = "red.pal", n1 = 3, n2 = 3),
           border = NA,
           colNA = "white",
           legend.pos = "topleft", legend.values.rnd = 2,
           legend.title.txt = "Résidus",
           add = TRUE)

layoutLayer(title = "Cartographie des résidus",
            author = "G. Bideau, 2024.",
            sources = "Sources : ")

typoLayer(x = df_reg, var = "COM_NOUV",
          legend.values.order = c("OUI", "NON"),
          legend.pos = "topleft",
          col=c("red","blue"),
          legend.title.txt = "Communes nouvelles",
          border = NA)

layoutLayer(title = "Communes nouvelles 2012-2024(01)",
            author = "G. Bideau, 2024.",
            sources = "Sources : ")

par(mar = c(0,0,0,0), mfrow = c(1,1))
```

## Test de Moran sur auto-corrélation spatiale des résidus (package rgeoda)

```{r LISA_rgeodata, cache=TRUE}

test_geom <- df_reg

# test_geom$COM_NOUV2<- if_else(test_geom$COM_NOUV == "OUI", 1, 0)
# variable <- "COM_NOUV2"
variable <- "residus"
nom_variable <- "Résidus"

df_variable <- test_geom[variable]

# Définition de la matrice de contiguïté
queen_w <- queen_weights(test_geom, order=1, include_lower_order = FALSE, precision_threshold = 0)

lisa <- local_moran(queen_w, df_variable)

# To get the values of the local Moran:
lms <- lisa_values(gda_lisa = lisa)

# To get the pseudo-p values of significance of local Moran computation:
pvals <- lisa_pvalues(lisa)

# To get the cluster indicators of local Moran computation:
cats <- lisa_clusters(lisa, cutoff = 0.05)

# Labels
lbls <- lisa_labels(lisa)


test_geom$LISA <- lms
test_geom$pvals <- pvals
test_geom$cats <- cats

# On rend les catégories plus lisibles en remplaçant le numéro par l'intitulé
test_geom$cats <- as.factor(test_geom$cats)
num_levels <- as.numeric(levels(test_geom$cats))
levels(test_geom$cats) <- lbls[num_levels + 1]
print(levels(test_geom$cats))

test_geom$LISA_retenu <- ifelse(test_geom$pvals <= 0.1, yes = test_geom$LISA, no = NA)

par(mar = c(0,0,0,0), mfrow = c(1,2))

couleurs <- c("#f7f7f7", "#e41a1c","#377eb8","#4daf4a","#984ea3","#ff7f00","#ffff33")
# Cf. https://colorbrewer2.org/#type=qualitative&scheme=Set1&n=6 pour composer les palettes
typoLayer(x = test_geom , var = "cats",
           # col = carto.pal(pal1 = "blue.pal", pal2 = "red.pal", n1 = 3, n2 = 3),
           col = couleurs[1:length(levels(test_geom$cats))],
           border = NA,
           legend.pos = "topleft",
           legend.values.order = levels(test_geom$cats),
           legend.title.txt = paste0("Indice de Moran locaux significatifs\nsur la variable « ", nom_variable, " »\n(pvalue < 0,1)"))


typoLayer(x = test_geom, var = "COM_NOUV",
          legend.values.order = c("OUI", "NON"),
          col=c("red","blue"),
          border = NA)

# plot(test_geomCN$geometry, col = NA, border = "black", lwd = 1, add = TRUE)

par(mar = c(0,0,0,0), mfrow = c(1,1))

rm(variable, df_variable, lms, pvals, cats, lbls, num_levels, couleurs)

```

# [À vérifier, tester] *Forward stepwise* ou *backward stepwise*

Possibilité de sélectionner automatiquement les variables à choisir pour une régression logistique [feuillet2019] :

- régression pas à pas ascendante (on ajoute des variables à partir d'un tableau de données) : *forward stepwise*  ;

- régression pas à pas descendante (on enlève) : *backward stepwise*.

Guides pour mettre en place ces méthodes : https://www.statology.org/stepwise-regression-r/ pour une présentation un peu rapide ou (déroulé suivi ici) : https://rstudio-pubs-static.s3.amazonaws.com/205694_3b195f29e9504d23aeb483ff1ffafeba.html, en particulier à partir du point 3.4.

## Sélection des données

```{r selection data_step_by_step}

# Sélection d'une partie de df_reg pour le step by step

variables_sbs <- c(colnames(df_reg[which(str_detect(colnames(df_reg), "_prct") == TRUE)]),
                            colnames(df_reg[which(str_detect(colnames(df_reg), "_RT") == TRUE)]))
variables_sbs <- c(colnames(df_reg[which(str_detect(colnames(df_reg), "_prct") == TRUE)]),
                            colnames(df_reg[which(str_detect(colnames(df_reg), "_RT") == TRUE)]), "CATAEU2010", "CODE_DEPT")
variables_sbs <- c(variables_sbs, variables_elect)
variables_sbs <- variables_sbs[variables_sbs!= "geometry"] # On supprime la variable "geometry" qui est intégrée sans que cela soit voulu

# On ne conserve que les variables qu'on a validé du point de vue du risque d'auto-corrélation
variables_sbs <- intersect(variables_sbs, variables_correlations) 

df_reg_sbs <- df_reg[, c("COM_NOUV", variables_sbs)]

df_reg_sbs <- na.omit(df_reg_sbs)

# Suppression des géométries
df_reg_sbs <- st_set_geometry (df_reg_sbs, NULL)

```

## Régression pas à pas 

```{r stepwise, warning = FALSE, message = FALSE}

# Pour procéder avec la sélection ascendante (Forward selection) on doit d’abord spécifier le modèle de départ
m0 <- glm(COM_NOUV ~ 1, data=df_reg_sbs, family = binomial(logit), na.action = na.exclude)  # choix du modèle avec constante seulement
# m0

# Pour utiliser la commande step on doit spécifier le modèle de départ m0 et le modèle maximal mf qui peut être le modèle complet.
mf <- glm(COM_NOUV ~ ., data=df_reg_sbs, family = binomial(logit), na.action = na.exclude)  # choix du modèle avec constante seulement
# mf

# La sélection ascendante utilisant le critère AIC
# step(m0, scope=list(lower=m0, upper=mf), data = df_reg_sbs, direction="forward")
reg_sbs_forward <- step(m0, scope=list(lower=m0, upper=mf), data = df_reg_sbs, direction="forward", trace = FALSE)
add_vif(tbl_regression(reg_sbs_forward, exponentiate = TRUE)) # Pour afficher les VIF

# La sélection descendante
# step(mf, data = df_reg_sbs, direction="backward")
reg_sbs_backward <- step(mf, data = df_reg_sbs, direction="backward", na.action = na.omit, trace = FALSE)
add_vif(tbl_regression(reg_sbs_backward, exponentiate = TRUE)) # Pour afficher les VIF

# La sélection pas a pas dans les deux sens
# step(m0, scope = list(upper=mf), data = df_reg_sbs,direction="both")
reg_sbs_both <- step(m0, scope = list(upper=mf), data = df_reg_sbs,direction="both", trace = FALSE)
add_vif(tbl_regression(reg_sbs_both, exponentiate = TRUE)) # Pour afficher les VIF

# La sélection ascendante utilisant le F−test des modèles emboîtés.
# step(m0, scope=list(lower=m0, upper=mf), data = df_reg_sbs, direction="forward",test="F")
reg_sbs_forward_F <- step(m0, scope=list(lower=m0, upper=mf), data = df_reg_sbs, direction="forward",test="F", trace = FALSE)
add_vif(tbl_regression(reg_sbs_forward_F, exponentiate = TRUE)) # Pour afficher les VIF

# La sélection descendante avec le F−test
# step(mf, data = df_reg_sbs, direction="backward")
reg_sbs_backward_F <- step(mf, data = df_reg_sbs, direction="backward", trace = FALSE)
add_vif(tbl_regression(reg_sbs_backward_F, exponentiate = TRUE)) # Pour afficher les VIF

# La sélection pas à pas dans les deux sens avec le F−test
# step(m0, scope = list(upper=mf), data = df_reg_sbs, direction="both")
reg_sbs_both_F <- step(m0, scope = list(upper=mf), data = df_reg_sbs, direction="both", trace = FALSE)
add_vif(tbl_regression(reg_sbs_both_F, exponentiate = TRUE)) # Pour afficher les VIF

```

## Algorithme génétique

Section 3.5 de la page déjà citée : https://rstudio-pubs-static.s3.amazonaws.com/205694_3b195f29e9504d23aeb483ff1ffafeba.html#lalgorithme-genetique

Mais ne fonctionne pas : problème pour installer glmulti sur RStudio Server + message d'erreur "!Oversized candidate set."

```{r utilis_glmulti, eval=FALSE}
df_reg_gen <- df_reg_sbs[1:50,1:10]
res3 <- glmulti(COM_NOUV~.,data = df_reg_gen,level = 2,method = "g",fitfunction = lm,crit = 'aic',plotty = F)

```




# Bibliographie


